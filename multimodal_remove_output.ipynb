{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logit lens"
      ],
      "metadata": {
        "id": "uTvdnps5ZYA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Содержание\n",
        "* Описание метода logit lens\n",
        "* Применение к модели"
      ],
      "metadata": {
        "id": "x-JKS_FzZbBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, LlavaForConditionalGeneration\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import random\n",
        "import os\n",
        "import random\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "gRrL5XTsVB59"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Описание метода logit lens\n",
        "Logit Lens используется для оценки промежуточных предсказаний трансформера. После каждого слоя берется скрытое состояние (hidden state), к нему применяется финальная нормализация слоя (если она есть), затем выходная линейная голова​, используемая при обычном предсказании токенов. После применения софтмакса получается распределение вероятности слов из словаря.Это позволяет узнать, какие токены были бы выбраны моделью, если бы предсказание делалось на этом этапе. Применение финальной головы позволяет понять насколько на данном этапе модель делает хорошие предсказания, нормализация применяется тк без неё масштаб будет отличаться от выходного слоя и приведет к некорректным результатам.\n",
        "Этот метод показывает как меняются логиты из более зашумленных на ранних слоях к более точным на поздних. Однако скрытые состояния промежуточных слоёв могут не лежать в том же линейном пространстве, что и выходной слой. Поэтому полученные логиты могут быть неточными, в таких случаях возможна ошибка интерпретации, так как промежуточные слои могут использовать другие представления. Просто алгоритма является и плюсом и минусом, было показано что метод не может сделать предсказания на хорошем уровне для моделей GPT-Neo, BLOOM, OPT 125M, плохо работает с нестандартными блоками.\n"
      ],
      "metadata": {
        "id": "yP7EcOA2Zp15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Датасеты\n",
        "Для применения мультимодальных моделей возьмём два датасета с изображениями, различающиеся по сложности и содержанию.\n",
        "1. CLEVR\n",
        "2. COCO (Common Objects in Context)"
      ],
      "metadata": {
        "id": "CHJ4C5_sunIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# код загрузки датасета CLEVR\n",
        "# далее 500 картинок были сохранены для более удобного использования\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"timoboz/clevr-dataset\")\n",
        "\n",
        "val_dir = os.path.join(path, \"CLEVR_v1.0\", \"images\", \"val\")\n",
        "all_images = sorted(glob.glob(os.path.join(val_dir, \"*.png\")))\n",
        "\n",
        "selected_images = random.sample(all_images, 500)\n",
        "output_dir = \"clevr_val_subset_500\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "for src_path in selected_images:\n",
        "    filename = os.path.basename(src_path)\n",
        "    shutil.copy(src_path, os.path.join(output_dir, filename))\n",
        "shutil.make_archive(\"clevr_val_subset_500\", \"zip\", \"clevr_val_subset_500\") # если нужно сорхранить локально"
      ],
      "metadata": {
        "id": "EbGa1_k2ain4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# датасет COCO был скачен по ссылке https://cocodataset.org/#download версия val2017\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "zip_path = \"val2017.zip\"\n",
        "extracted_dir = \"val2017_full/val2017\"\n",
        "subset_dir = \"coco_val_500\"\n",
        "\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "image_files = [f for f in os.listdir(extracted_dir) if f.endswith(\".jpg\")]\n",
        "random.seed(42)\n",
        "selected = random.sample(image_files, 500)\n",
        "\n",
        "os.makedirs(subset_dir, exist_ok=True)\n",
        "for fname in selected:\n",
        "    shutil.copy(os.path.join(extracted_dir, fname), os.path.join(subset_dir, fname))\n",
        "shutil.make_archive(\"coco_val_500\", \"zip\", \"coco_val_500\") # если нужно сорхранить локально"
      ],
      "metadata": {
        "id": "ILnuilN4fqKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# при загрузке из зип файлов\n",
        "import zipfile\n",
        "\n",
        "def unzip_file(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "unzip_file(\"clevr_val_subset_500.zip\", \"clevr_dataset\")\n",
        "unzip_file(\"coco_val_500.zip\", \"coco_dataset\")"
      ],
      "metadata": {
        "id": "EXCBQS9TreRw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Модели\n",
        "Для анализа было выбрано 3 модели:\n",
        "1. llava-onevision на 1.5b\n",
        "2. qwen2-vl на 2b\n",
        "\n",
        "Они имеют схожую структуру:\n",
        "* Визуальный модуль- разбивает изображение на патчи, добавляет позиционный энкодинг, преобразует изображение в эмбеддинги слоями трансформера (CLIPVisionTransformer для LLaVA-OneVision, VisionTransformer для Qwen2-VL)\n",
        "\n",
        "* Мультимодальная проекция - приводит эмбеддинги изображений к нужной текстовой размерности\n",
        "\n",
        "* Языковая модель, блок который будет использоваться с помощью метода logit lens\n",
        "  * LLaMa (0-31)\n",
        "  * Qwen2-2B (0-23)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lgvg4yQpQzZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Исследование динамики модели\n",
        "Для каждого датасета возьмём 5 картинок и визуально посмотрим на изменение токенов на каждом датасете и уверенность модели\n"
      ],
      "metadata": {
        "id": "mrBwQGXwZlKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### llava-onevision на 1.5b"
      ],
      "metadata": {
        "id": "2ktpKxsoZ3Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка модели\n",
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "model_llava = LlavaForConditionalGeneration.from_pretrained(\n",
        "  model_id,\n",
        "  torch_dtype=torch.float16,\n",
        "  low_cpu_mem_usage=True,\n",
        ").to(0)\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "3_MUuGizqltz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hidden_states(prompt, img, model):\n",
        "    # Обработка текстового запроса\n",
        "    inputs = processor.apply_chat_template(prompt, add_generation_prompt=True)\n",
        "    inputs = processor(text=inputs, images=img, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # сохранение логитов на каждом шаге\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            **inputs,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "        hidden_states = outputs.hidden_states\n",
        "    return hidden_states, inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "# При визуализации нескольких топ токенов возникала проблема, что split() превращал\n",
        "# специальные токены в пустые строки\n",
        "def safe_decode(token_id: int, tokenizer) -> str:\n",
        "\n",
        "    raw = tokenizer.decode([token_id], skip_special_tokens=False)\n",
        "\n",
        "    if token_id in tokenizer.all_special_ids or raw in tokenizer.all_special_tokens:\n",
        "        return f\"<{raw.strip() or 'SPECIAL'}>\"\n",
        "\n",
        "    if raw.strip() == \"\":\n",
        "        if raw == \" \":\n",
        "            return \"<SPACE>\"\n",
        "        elif raw == \"\\n\":\n",
        "            return \"<NEWLINE>\"\n",
        "        elif raw == \"\":\n",
        "            return \"<EMPTY>\"\n",
        "\n",
        "    return raw\n",
        "\n",
        "def get_visuals(hidden_states, img, model, len_inputs):\n",
        "    # подготовим данные к визуализации\n",
        "    # уверенность модели в первом токене\n",
        "    # топ-3 токенов\n",
        "    target_pos = len_inputs - 1\n",
        "\n",
        "    layer_confidences = []\n",
        "    top_tokens = []\n",
        "    seq_len = hidden_states[-1].shape[1]\n",
        "    target_positions = list(range(seq_len - 3, seq_len))\n",
        "    for i, hs in enumerate(hidden_states):\n",
        "        logits =  model.lm_head(model.language_model.norm(hs))\n",
        "        max_probs = torch.softmax(logits, dim=-1).max(dim=-1).values\n",
        "        layer_confidences.append(max_probs.mean().item())\n",
        "\n",
        "        topk = torch.topk(logits, k=3, dim=-1)\n",
        "        top_ids = topk.indices[0, target_pos]\n",
        "        top_strs = [safe_decode(i.item(), processor.tokenizer) for i in top_ids]\n",
        "        top_tokens.append(top_strs)\n",
        "        del logits\n",
        "        del max_probs\n",
        "\n",
        "    # визуализация\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.lineplot(x=list(range(len(layer_confidences))), y=layer_confidences, marker=\"o\")\n",
        "    plt.xlabel(\"Layer index\")\n",
        "    plt.ylabel(\"Average max softmax probability\")\n",
        "    plt.title(\"Logit Lens Confidence Across Layers\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    table_data = [\n",
        "        [f\"Layer {i}\", t[0], t[1], t[2]] for i, t in enumerate(top_tokens)\n",
        "    ]\n",
        "    col_labels = [\"Layer\", \"Top-1\", \"Top-2\", \"Top-3\"]\n",
        "\n",
        "    table = ax.table(\n",
        "        cellText=table_data,\n",
        "        colLabels=col_labels,\n",
        "        loc='center',\n",
        "        cellLoc='left'\n",
        "    )\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1.1, 1.2)\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d7P6HlKbssCc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "clevr_folder = \"/content/clevr_dataset\"\n",
        "coco_folder = \"/content/coco_dataset\"\n",
        "\n",
        "clevr_paths = random.sample(\n",
        "    [os.path.join(clevr_folder, f) for f in os.listdir(clevr_folder) if f.endswith(\".png\")], 5)\n",
        "\n",
        "coco_paths = random.sample(\n",
        "    [os.path.join(coco_folder, f) for f in os.listdir(coco_folder) if f.endswith(\".jpg\")], 5)\n",
        "\n",
        "clevr_images = [Image.open(p).convert(\"RGB\") for p in clevr_paths]\n",
        "coco_images = [Image.open(p).convert(\"RGB\") for p in coco_paths]"
      ],
      "metadata": {
        "id": "z_x4EgzmNMRN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clevr_prompts = [\n",
        "    \"Color of the main object:\",\n",
        "    \"Shape of the main object:\",\n",
        "    \"Material of the main object:\",\n",
        "    \"Number of main objects:\",\n",
        "]\n",
        "\n",
        "coco_prompts = [\n",
        "    \"Main object:\",\n",
        "    \"Answer with one word. What is the person doing:\",\n",
        "    \"Place:\",\n",
        "    \"Main animal:\",\n",
        "    \"Answer with one word. What is shown in the background:\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "for dataset_name, image_list, prompts in [(\"CLEVR\", clevr_images, clevr_prompts), (\"COCO\", coco_images, coco_prompts)]:\n",
        "    for i, img in enumerate(image_list):\n",
        "        print(f\"\\n===== {dataset_name} Image {i+1} =====\")\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{dataset_name} Image {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "        for j, prompt_text in enumerate(prompts):\n",
        "            print(f\"\\n--- Prompt {j+1}: {prompt_text} ---\")\n",
        "            prompt = [{\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\", \"data\": img},\n",
        "                {\"type\": \"text\", \"text\": prompt_text}\n",
        "            ]}]\n",
        "            hidden_states, len_inputs = get_hidden_states(prompt, img, model_llava)\n",
        "            get_visuals(hidden_states, img, model_llava, len_inputs)\n",
        "            del hidden_states"
      ],
      "metadata": {
        "id": "7ExTINOUo8sW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Эксперименты с изменёнными запросами\n",
        "clevr_prompts = [\n",
        "    \"Color of the smaller object:\",\n",
        "    \"Answer using one word. 3D Shape of the main object:\",\n",
        "    \"Answer using one word. Shape of the object to the left of the cube:\",\n",
        "]\n",
        "\n",
        "for dataset_name, image_list, prompts in [(\"CLEVR\", clevr_images, clevr_prompts), (\"COCO\", coco_images, coco_prompts)]:\n",
        "    for i, img in enumerate(image_list):\n",
        "        print(f\"\\n===== {dataset_name} Image {i+1} =====\")\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{dataset_name} Image {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "        for j, prompt_text in enumerate(prompts):\n",
        "            print(f\"\\n--- Prompt {j+1}: {prompt_text} ---\")\n",
        "            prompt = [{\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\", \"data\": img},\n",
        "                {\"type\": \"text\", \"text\": prompt_text}\n",
        "            ]}]\n",
        "            hidden_states, len_inputs = get_hidden_states(prompt, img, model_llava)\n",
        "            get_visuals(hidden_states, img, model_llava, len_inputs)\n",
        "            del hidden_states"
      ],
      "metadata": {
        "id": "fwg8HGmCCvK4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_llava"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-HrmVrj6Yje",
        "outputId": "c6bf6041-76a8-47c3-c994-813b9580fcfa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlavaForConditionalGeneration(\n",
              "  (model): LlavaModel(\n",
              "    (vision_tower): CLIPVisionModel(\n",
              "      (vision_model): CLIPVisionTransformer(\n",
              "        (embeddings): CLIPVisionEmbeddings(\n",
              "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
              "          (position_embedding): Embedding(577, 1024)\n",
              "        )\n",
              "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (encoder): CLIPEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-23): 24 x CLIPEncoderLayer(\n",
              "              (self_attn): CLIPAttention(\n",
              "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              )\n",
              "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (mlp): CLIPMLP(\n",
              "                (activation_fn): QuickGELUActivation()\n",
              "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "              )\n",
              "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (multi_modal_projector): LlavaMultiModalProjector(\n",
              "      (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "      (act): GELUActivation()\n",
              "      (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    )\n",
              "    (language_model): LlamaModel(\n",
              "      (embed_tokens): Embedding(32064, 4096)\n",
              "      (layers): ModuleList(\n",
              "        (0-31): 32 x LlamaDecoderLayer(\n",
              "          (self_attn): LlamaAttention(\n",
              "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          )\n",
              "          (mlp): LlamaMLP(\n",
              "            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
              "            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
              "            (act_fn): SiLU()\n",
              "          )\n",
              "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        )\n",
              "      )\n",
              "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      (rotary_emb): LlamaRotaryEmbedding()\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qwen"
      ],
      "metadata": {
        "id": "q29a525TUOix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_qwen = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
      ],
      "metadata": {
        "id": "2iHTrOuAVCx-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clevr_prompts = [\n",
        "    \"Color of the main object:\",\n",
        "    \"Shape of the main object:\",\n",
        "    \"Material of the main object:\",\n",
        "    \"Number of main objects:\",\n",
        "]\n",
        "\n",
        "coco_prompts = [\n",
        "    \"Main object:\",\n",
        "    \"Answer with one word. What is the person doing:\",\n",
        "    \"Place:\",\n",
        "    \"Main animal:\",\n",
        "    \"Answer with one word. What is shown in the background:\"\n",
        "]\n",
        "\n",
        "for dataset_name, image_list, prompts in [(\"CLEVR\", clevr_images, clevr_prompts), (\"COCO\", coco_images, coco_prompts)]:\n",
        "    for i, img in enumerate(image_list):\n",
        "        print(f\"\\n===== {dataset_name} Image {i+1} =====\")\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{dataset_name} Image {i+1}\")\n",
        "        plt.show()\n",
        "\n",
        "        for j, prompt_text in enumerate(prompts):\n",
        "            print(f\"\\n--- Prompt {j+1}: {prompt_text} ---\")\n",
        "            prompt = [{\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\", \"data\": img},\n",
        "                {\"type\": \"text\", \"text\": prompt_text}\n",
        "            ]}]\n",
        "            hidden_states, len_inputs = get_hidden_states(prompt, img, model_qwen)\n",
        "            get_visuals(hidden_states, img, model_qwen, len_inputs)\n",
        "            del hidden_states"
      ],
      "metadata": {
        "id": "LKll4p6HVZ-q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_qwen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg5a5P3rWfUN",
        "outputId": "78a8a6c0-3a55-4509-f7a1-e471f1b009d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2VLForConditionalGeneration(\n",
              "  (model): Qwen2VLModel(\n",
              "    (visual): Qwen2VisionTransformerPretrainedModel(\n",
              "      (patch_embed): PatchEmbed(\n",
              "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
              "      )\n",
              "      (rotary_pos_emb): VisionRotaryEmbedding()\n",
              "      (blocks): ModuleList(\n",
              "        (0-31): 32 x Qwen2VLVisionBlock(\n",
              "          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "          (attn): VisionAttention(\n",
              "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (mlp): VisionMlp(\n",
              "            (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (act): QuickGELUActivation()\n",
              "            (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (merger): PatchMerger(\n",
              "        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (language_model): Qwen2VLTextModel(\n",
              "      (embed_tokens): Embedding(151936, 1536)\n",
              "      (layers): ModuleList(\n",
              "        (0-27): 28 x Qwen2VLDecoderLayer(\n",
              "          (self_attn): Qwen2VLAttention(\n",
              "            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "            (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
              "            (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
              "            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
              "            (rotary_emb): Qwen2VLRotaryEmbedding()\n",
              "          )\n",
              "          (mlp): Qwen2MLP(\n",
              "            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
              "            (act_fn): SiLU()\n",
              "          )\n",
              "          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "        )\n",
              "      )\n",
              "      (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "      (rotary_emb): Qwen2VLRotaryEmbedding()\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ssOdhg75eexv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}